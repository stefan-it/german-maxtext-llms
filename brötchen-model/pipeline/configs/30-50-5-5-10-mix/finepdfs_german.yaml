tokenizer:
  name: "german-maxtext-slms/broetchen-ablation-1-tokenizer"

array_record:
  safe_after_n_documents: 100_000

datasets:
  finepdfs_german:
    hf_data_files: "data/deu_Latn/train/*.parquet"
    max_subtokens: 100_000_000_000
    hf_identifier: "HuggingFaceFW/finepdfs-edu"
    output_path: "./30-50-5-5-10-mix/finepdfs_german"
