# German MaxText SLMs

This repository contains training code for pretraining small German language models from scratch on TPUs using the [MaxText](https://github.com/AI-Hypercomputer/maxtext) library.

## Features

- Full pretraining pipeline for German SLMs on TPU infrastructure
- Recipes for training various tokenizers optimized for German
- Comprehensive tokenizer evaluation across multiple metrics

## Status

This is an active project with regular updates. Check back frequently for new features and improvements.

## Changelog

* 02.12.2025: Release of pretraining book for the nano model.
* **15.11.2025**: Initial release

## nano Model

We trained a nano model with 176M parameters. The training playbook can be found [here](nano-model/README.md).

## ‚ù§Ô∏è Acknowledgements

Huge thanks to Google for providing TPU resources through the [TPU Research Cloud (TRC) program](https://sites.research.google/trc/about/)!

---

Made from Bavarian Oberland with ‚ù§Ô∏è and ü•®.
