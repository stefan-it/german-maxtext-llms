# German MaxText LLMs

This repository contains training code for pretraining German language models from scratch on TPUs using the [MaxText](https://github.com/AI-Hypercomputer/maxtext) library.

## Features

- Full pretraining pipeline for German LLMs on TPU infrastructure
- Recipes for training various tokenizers optimized for German
- Comprehensive tokenizer evaluation across multiple metrics

## Status

This is an active project with regular updates. Check back frequently for new features and improvements.

## Changelog

* **15.11.2025**: Initial release

## ‚ù§Ô∏è Acknowledgements

Huge thanks to Google for providing TPU resources through the [TPU Research Cloud (TRC) program](https://sites.research.google/trc/about/)!

---

Made from Bavarian Oberland with ‚ù§Ô∏è and ü•®.
