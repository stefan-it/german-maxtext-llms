[project]
name = "german-maxtext-llms"
version = "0.1.0"
description = "Building German MaxText LLMs"
readme = "README.md"
requires-python = ">=3.10"
license = { text = "MIT" }
authors = [
    { name = "Stefan Schweter", email = "stefan@schweter.it" }
]
keywords = ["tokenizer", "german", "llm", "nlp"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]

dependencies = [
    "blobfile>=3.0.0",
    "pyarrow>=21.0.0",
    "tiktoken>=0.11.0",
    "torch>=2.8.0",
    "transformers>=4.55.0",
]

[build-system]
requires = ["maturin>=1.7,<2.0"]
build-backend = "maturin"

[tool.maturin]
module-name = "rustbpe"
bindings = "pyo3"
python-source = "."
manifest-path = "rustbpe/Cargo.toml"

[dependency-groups]
dev = [
    "maturin>=1.9.4",
]

[project.urls]
Homepage = "https://github.com/stefan-it/german-maxtext-llms"
Repository = "https://github.com/stefan-it/german-maxtext-llms"
Issues = "https://github.com/stefan-it/german-maxtext-llms/issues"

# target torch to cuda 12.8 or CPU
[tool.uv.sources]
torch = [
    { index = "pytorch-cpu", extra = "cpu" },
    { index = "pytorch-cu128", extra = "gpu" },
]

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true
  
[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

[project.optional-dependencies]
cpu = [
    "torch>=2.8.0",  
]
gpu = [
    "torch>=2.8.0",
]

[tool.uv]
conflicts = [
    [
        { extra = "cpu" },
        { extra = "gpu" },
    ],
]